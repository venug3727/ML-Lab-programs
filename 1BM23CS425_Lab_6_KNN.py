# -*- coding: utf-8 -*-
"""1BM23CS425_Lab-6-KNN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1R1WCTKlpHnapT05gTZLwJCIwjCCQDWES
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
import seaborn as sns
import matplotlib.pyplot as plt

# Load the Iris dataset
iris = pd.read_csv("/content/iris (2).csv")

# Check the first few rows of the dataset
print(iris.head())

# Split the data into features (X) and target (y)
X_iris = iris.drop('species', axis=1)
y_iris = iris['species']

# Split the data into training and testing sets (80% training, 20% testing)
X_train_iris, X_test_iris, y_train_iris, y_test_iris = train_test_split(
    X_iris, y_iris, test_size=0.2, random_state=42)

# =============================================
# NEW: Automated k-value selection using cross-validation
# =============================================
k_values = range(1, 30)
cv_scores = []

for k in k_values:
    knn = KNeighborsClassifier(n_neighbors=k)
    scores = cross_val_score(knn, X_train_iris, y_train_iris, cv=5, scoring='accuracy')
    cv_scores.append(scores.mean())

# Find the k with highest cross-validation accuracy
optimal_k = k_values[np.argmax(cv_scores)]
print(f"\nOptimal k value determined: {optimal_k}")

# Plot accuracy vs k values
plt.figure(figsize=(10, 6))
plt.plot(k_values, cv_scores, 'bo-')
plt.xlabel('k Value')
plt.ylabel('Cross-Validated Accuracy')
plt.title('Finding Optimal k Value')
plt.axvline(x=optimal_k, color='r', linestyle='--')
plt.show()
# =============================================

# Train final model with optimal k
knn_iris = KNeighborsClassifier(n_neighbors=optimal_k)
knn_iris.fit(X_train_iris, y_train_iris)

# Make predictions on the test data
y_pred_iris = knn_iris.predict(X_test_iris)

# Evaluate the model
accuracy_iris = accuracy_score(y_test_iris, y_pred_iris)
conf_matrix_iris = confusion_matrix(y_test_iris, y_pred_iris)
class_report_iris = classification_report(y_test_iris, y_pred_iris)

# Print results for Iris dataset
print(f"\nAccuracy for Iris dataset: {accuracy_iris:.4f}")
print(f"\nConfusion Matrix for Iris dataset:\n{conf_matrix_iris}")
print(f"\nClassification Report for Iris dataset:\n{class_report_iris}")

# Plot confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix_iris, annot=True, fmt='d', cmap='Blues',
            xticklabels=iris['species'].unique(),
            yticklabels=iris['species'].unique())
plt.title("Confusion Matrix for Iris Dataset")
plt.xlabel('Predicted')
plt.ylabel('True')
plt.show()

# New data for prediction
new_flower_1 = [[5.1, 3.5, 1.4, 0.2]]  # Likely setosa
new_flower_2 = [[6.7, 3.0, 5.0, 2.3]]  # Likely virginica

# Make predictions
prediction_1 = knn_iris.predict(new_flower_1)
prediction_2 = knn_iris.predict(new_flower_2)

print(f"\nPrediction for flower [5.1, 3.5, 1.4, 0.2]: {prediction_1[0]}")
print(f"Prediction for flower [6.7, 3.0, 5.0, 2.3]: {prediction_2[0]}")

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt

# Range of k values to test
k_values = range(1, 30)
accuracies = []

for k in k_values:
    knn = KNeighborsClassifier(n_neighbors=k)
    knn.fit(X_train, y_train)
    y_pred = knn.predict(X_test)
    accuracies.append(accuracy_score(y_test, y_pred))

# Plot k vs accuracy
plt.figure(figsize=(10, 6))
plt.plot(k_values, accuracies, 'bo-')
plt.xlabel('k')
plt.ylabel('Accuracy')
plt.title('Finding Optimal k (Elbow Method)')
plt.show()

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
import seaborn as sns
import matplotlib.pyplot as plt

# Load the Diabetes dataset
diabetes = pd.read_csv("/content/diabetes.csv")

# Check the first few rows of the dataset
print(diabetes.head())

# Split the data into features (X) and target (y)
X_diabetes = diabetes.drop('Outcome', axis=1)
y_diabetes = diabetes['Outcome']

# Split the data into training and testing sets (80% training, 20% testing)
X_train_diabetes, X_test_diabetes, y_train_diabetes, y_test_diabetes = train_test_split(
    X_diabetes, y_diabetes, test_size=0.2, random_state=42)

# Perform feature scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_diabetes)
X_test_scaled = scaler.transform(X_test_diabetes)

# =============================================
# Automated k-value selection using cross-validation
# =============================================
k_values = range(1, 30)
cv_scores = []

for k in k_values:
    knn = KNeighborsClassifier(n_neighbors=k)
    scores = cross_val_score(knn, X_train_scaled, y_train_diabetes, cv=5, scoring='accuracy')
    cv_scores.append(scores.mean())

# Find the k with highest cross-validation accuracy
optimal_k = k_values[np.argmax(cv_scores)]
print(f"\nOptimal k value determined: {optimal_k}")

# Plot accuracy vs k values
plt.figure(figsize=(10, 6))
plt.plot(k_values, cv_scores, 'bo-')
plt.xlabel('k Value')
plt.ylabel('Cross-Validated Accuracy')
plt.title('Finding Optimal k Value for Diabetes Dataset')
plt.axvline(x=optimal_k, color='r', linestyle='--')
plt.show()
# =============================================

# Train final model with optimal k
knn_diabetes = KNeighborsClassifier(n_neighbors=optimal_k)
knn_diabetes.fit(X_train_scaled, y_train_diabetes)

# Make predictions on the test data
y_pred_diabetes = knn_diabetes.predict(X_test_scaled)

# Evaluate the model
accuracy_diabetes = accuracy_score(y_test_diabetes, y_pred_diabetes)
conf_matrix_diabetes = confusion_matrix(y_test_diabetes, y_pred_diabetes)
class_report_diabetes = classification_report(y_test_diabetes, y_pred_diabetes)

# Print results for Diabetes dataset
print(f"\nAccuracy for Diabetes dataset: {accuracy_diabetes:.4f}")
print(f"\nConfusion Matrix for Diabetes dataset:\n{conf_matrix_diabetes}")
print(f"\nClassification Report for Diabetes dataset:\n{class_report_diabetes}")

# Plot confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix_diabetes, annot=True, fmt='d', cmap='Blues',
            xticklabels=['No Diabetes', 'Diabetes'],
            yticklabels=['No Diabetes', 'Diabetes'])
plt.title("Confusion Matrix for Diabetes Dataset")
plt.xlabel('Predicted')
plt.ylabel('True')
plt.show()

# Example prediction
new_patient = [[6, 148, 72, 35, 0, 33.6, 0.627, 50]]  # Example patient data
new_patient_scaled = scaler.transform(new_patient)
prediction = knn_diabetes.predict(new_patient_scaled)
print(f"\nPrediction for new patient: {'Diabetes' if prediction[0] == 1 else 'No Diabetes'}")

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt

# Range of k values to test
k_values = range(1, 30)
accuracies = []

for k in k_values:
    knn = KNeighborsClassifier(n_neighbors=k)
    knn.fit(X_train_diabetes, y_train_diabetes)
    y_pred = knn.predict(X_test_diabetes)
    accuracies.append(accuracy_score(y_test_diabetes, y_pred))

# Plot k vs accuracy
plt.figure(figsize=(10, 6))
plt.plot(k_values, accuracies, 'bo-')
plt.xlabel('k')
plt.ylabel('Accuracy')
plt.title('Finding Optimal k (Elbow Method)')
plt.show()

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import (accuracy_score, confusion_matrix,
                           classification_report, f1_score)

# Load the dataset
heart = pd.read_csv('/content/heart.csv')

# Display basic info
print("Dataset Info:")
print(heart.info())
print("\nFirst 5 rows:")
print(heart.head())

# Split features and target
X = heart.drop('target', axis=1)
y = heart['target']

# Split data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42)

# Feature scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Find optimal k value
k_values = range(1, 30)
cv_scores = []
test_accuracies = []

for k in k_values:
    knn = KNeighborsClassifier(n_neighbors=k)
    # 5-fold cross validation
    scores = cross_val_score(knn, X_train_scaled, y_train, cv=5, scoring='accuracy')
    cv_scores.append(scores.mean())

    # Also track test accuracy for comparison
    knn.fit(X_train_scaled, y_train)
    y_pred = knn.predict(X_test_scaled)
    test_accuracies.append(accuracy_score(y_test, y_pred))

# Get optimal k
optimal_k = k_values[np.argmax(cv_scores)]
max_accuracy = max(cv_scores)
print(f"\nOptimal k value: {optimal_k} with cross-validation accuracy: {max_accuracy:.4f}")

# Plot accuracy vs k
plt.figure(figsize=(12, 6))
plt.plot(k_values, cv_scores, 'bo-', label='Cross-Validation Accuracy')
plt.plot(k_values, test_accuracies, 'go-', label='Test Accuracy')
plt.axvline(x=optimal_k, color='r', linestyle='--')
plt.xlabel('Number of Neighbors (k)')
plt.ylabel('Accuracy')
plt.title('Finding Optimal k Value')
plt.legend()
plt.show()

# Train final model with optimal k
final_knn = KNeighborsClassifier(n_neighbors=optimal_k)
final_knn.fit(X_train_scaled, y_train)
y_pred = final_knn.predict(X_test_scaled)

# Evaluation metrics
accuracy = accuracy_score(y_test, y_pred)
cm = confusion_matrix(y_test, y_pred)
cr = classification_report(y_test, y_pred)

print(f"\nFinal Model Accuracy: {accuracy:.4f}")
print("\nConfusion Matrix:")
print(cm)

# Plot confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['No Heart Disease', 'Heart Disease'],
            yticklabels=['No Heart Disease', 'Heart Disease'])
plt.title('Confusion Matrix for Heart Disease Prediction')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

print("\nClassification Report:")
print(cr)

# Example prediction
new_patient = [[63, 1, 3, 145, 233, 1, 0, 1, 0, 2, 0, 0, 1]]  # Example data
new_patient_scaled = scaler.transform(new_patient)
prediction = final_knn.predict(new_patient_scaled)
print(f"\nExample Prediction for New Patient: {'Heart Disease' if prediction[0] == 1 else 'No Heart Disease'}")